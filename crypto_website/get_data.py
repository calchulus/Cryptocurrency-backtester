from bs4 import BeautifulSoup
import requests
import pandas as pd
import pickle
import datetime
from selenium import webdriver
import time
import random
import json
import PyPDF2, re, os, csv
from nltk.tokenize import RegexpTokenizer
from os import listdir
from os.path import isfile, join
from nltk.stem.wordnet import WordNetLemmatizer
from textstat.textstat import textstat


NUM_COINS = 250


def all_coins():
    '''
    Requests a json file from the API, Cryptocompare, and filters for a
    dictionary of all the coins in Cryptocompare's database.

    Returns:
      coin_dict: a dictionary of all the coins in Cryptocompare's database
    '''

    url = "https://www.cryptocompare.com/api/data/coinlist/"
    r = requests.get(url).json()['Data']
    coin_dict = {}
    ticker_list = list(r.keys())
    for each in ticker_list:
        coin_dict[each] = {}
        coin_dict[each]["id"] = r[each]["Id"]
        coin_dict[each]["name"] = r[each]["CoinName"]
        coin_dict[each]["proof"] = r[each]["ProofType"]
        coin_dict[each]["supply"] = r[each]["TotalCoinSupply"]
    return coin_dict


def generate_url(id_code):
    '''
    Generates the url for the json file of the social media statistics for
    each coin in Crytocompare's database.

    Input:
      id_code (string): the unique id of a coin in Cryptocompare's database

    Returns:
      url (string): a string that contains the url for the json file of the
      social media statistics for the current coin
    '''

    base_url = "https://www.cryptocompare.com/api/data/socialstats/?id="
    url = base_url + id_code
    return url


def get_all_coins_reddit():
    '''
    Generates a list of the unique ids of each coin in Cryptocompare's
    database.

    Returns:
      ids (list): a list of the coin's unique ids
    '''

    ids = []
    coin_dict = all_coins()
    for key in coin_dict.keys():
        id_code = coin_dict[key]["id"]
        ids.append(id_code)
    return ids


def extract_reddit_num():
    '''
    Requests the url of the coin's json file with social media statistics and
    saves the total Reddit subscriber information into a dictionary with the
    ids of the coins as the keys and maps to a dictionary that contains the
    total Reddit subscriber statistics. Dumps this dictionary into a pickle
    file.
    '''

    d = {}
    id_ls = get_all_coins_reddit()
    for id_code in id_ls:
        url = generate_url(id_code)
        try:
            info = requests.get(url).json()["Data"]["Reddit"]
        except:
            pass
        d[id_code] = info
    pickle.dump(d, open("unclean_data.p", "wb"))


def clean_dictionary():
    '''
    Cleans up the dictionary generated by the function, extract_reddit_num.
    Creates a new dictionary that maps the coin names to the total number
    of Reddit subscribers the coin has. Dumps this new dictionary into a
    pickle file.
    '''

    unclean_dict = pickle.load(open("unclean_data.p", "rb"))
    coin_df = pd.read_csv("Updated_Cryptocoins.csv")
    all_coins = [x.lower() for x in coin_df["Name"]][0:250]
    new_dict = {}
    for key, val in unclean_dict.items():
        if val["Points"] == 0 or val["name"].lower() not in all_coins:
            continue
        new_dict[val["name"].lower()] = val["subscribers"]
    for coin in all_coins:
        if coin not in new_dict:
            new_dict[coin] = 0
    pickle.dump(new_dict, open("all_data.p", "wb"))


def create_non_time_df():
    '''
    Taking the clean dictionary generated by the function, clean_dictionary,
    this function creates a dataframe with three columns and inserts the
    number of total Reddit subscribers for each coin from the dictionary
    into the dataframe. The dataframe is saved as a csv file.
    '''

    new_dict = pickle.load(open("all_data.p", "rb"))
    coin_df = pd.read_csv("Updated_Cryptocoins.csv")
    all_coins = [x.lower() for x in coin_df["Name"]][0:250]
    zero_values = [0 for i in range(len(all_coins))]
    df_dict = {"Coin_Name": all_coins, "Total_Reddit_Subscribers": \
    zero_values, "White_Complexity": zero_values, "Twitter_Mentions": \
    zero_values}
    df = pd.DataFrame(df_dict)
    for i, coin in enumerate(all_coins):
        val = new_dict[coin]
        df.set_value(i, "Total_Reddit_Subscribers", val)
    df.to_csv("Static_Params.csv", index=False)


def coinmarketcap(filepath):
    '''
    In our case, we had a csv file from the beautiful soup
    of the all coins page on coinmarketcap.com
    (link = https://coinmarketcap.com/all/views/all/).
    This function takes the top 250 coins to create all of our searches.

    Input:
      filepath (string): filepath to a csv file.

    Returns:
      search_list: a list of search terms, one for each coin in the csv.
    '''

    df = pd.read_csv(filepath)
    search_list = []
    for each in df['Name']:
        search_term = each + " white paper filetype:pdf"
        search_list.append(search_term)
    return search_list


def search(search_string):
    '''
    Takes in a search string and gets the BeautifulSoup from Bing.
    We used bing because of no rate limiting. Google's API prevents us
    from querying Google easily. From these soups, we found if
    the coin's whitepaper showed up while accounting for errors if there
    were no results at all.

    Input:
      search_string: a string to search, in our case a coin name +
      "white paper filetype:pdf"

    Returns:
      link (string): link to coin's pdf or None if not found
    '''

    search_string.replace(' ','%20')
    search_string.replace(':', '%3A')
    bing_search = "https://www.bing.com/search?q=" + search_string
    r = requests.get(bing_search)
    if r is not None:
        soup = BeautifulSoup(r.text, "html.parser")
        a = soup.find_all("h1")
        if "There are no results for" in str(a[-1]):
            return None
        else:
            link = soup.find("h2").find("a")['href']
            return link


def many_search(search_list):
    '''
    Conducts the searches for all coins' whitepapers.

    Input:
      search_list: a list of the coins' queries for whitepapers

    Returns:
      links_list: a list of appropriate whitepaper links
    '''

    links_list = []
    for each in search_list:
        a = search(each)
        if a is None:
            continue
        else:
            if a not in links_list:
                links_list.append(a)
            else:
                links_list.append(0)
    return links_list


def pdf_writer(links_list, name_list):
    '''
    Writes/downloads all whitepaper pdfs into current folder.

    Inputs:
      links_list: a list of links to whitepapers
      name_list: a list of corresponding coin names
    '''

    for i in range(len(links_list)):
        if links_list[i] != 0:
            r = requests.get(links_list[i], stream=True)
            with open(name_list[i] + ".pdf", 'wb') as fd:
                for chunk in r.iter_content(2000):
                    fd.write(chunk)


def get_list_files(mypath):
    '''
    Finds all the txt files in a directory and returns a list of them
    Run this for the whitepaper text files folder
    Input:
      mypath (string): the filepath

    Returnss:
      coins_list: a list containing strings of all the coins
      (filenames minus extensions) in the folder
      paths: a list of all the strings of individual filepaths
    '''

    files_in_path = listdir(mypath)
    for file in files_in_path:
        convert(file)
    coins_list = [f[:-4] for f in files_in_path if isfile(join(mypath, f)) \
    and f[-4:] == ".txt"]
    paths = [mypath + f for f in files_in_path]
    return coins_list, paths


def convert(file):
    '''
    Converts PDFs to text files.

    Input:
      file (string): a filepath (if the file is a pdf, it writes
      a corresponding .txt file to the same directory)
    '''

    if file[-4:] == ".pdf":
        os.system('pdftotext ' + file)


def text(file):
    '''
    Gets the Flesch Kincaid grade level for the whitepapers.

    Input:
      file (string): the path to a txt file

    Returns:
      This function returns a Flesch Kincaid grade level. If file does not
      exist, the function returns a Flesch score of 0. Flesch Kincaid Grade
      levels often will be greater than 12 for college+ level papers.
    '''

    if os.path.isfile(file):
        with open(file, 'rb') as f:
            contents = f.read()
        contents.strip()
        z = contents.decode('utf-8', 'ignore')
        if len(z) != 0:
            grade = textstat.flesch_kincaid_grade(z)
            return grade
    else:
        return 0


def adv_text(file):
    '''
    We ultimately decided not to put this in the interface, but would be
    an interesting item to study along with the regular Flesch score itself
    Do more non-BS projects have a lower absolute value of difference between
    the flesch score of the real text vs. the flesch score of the frequent
    words. We have to do more testing to decide if this is a worthwhile stat
    to track

    Input:
      file (string): the path to a txt file

    Returns:
      This function returns the difficulty of the most common words in a
      whitepaper.
    '''

    lmtzr = WordNetLemmatizer()
    if os.path.isfile(file):
        with open(file, 'rb') as f:
            contents = f.read()
        contents.strip()
        z = contents.decode('utf-8', 'ignore')
        tokenizer = RegexpTokenizer(r'\w+')
        tokens = tokenizer.tokenize(z)
        length = len(tokens)
        count = {}
        freq_dict = {}
        freq_list = []
        for token in tokens:
            word = lmtzr.lemmatize(token)
            if len(word) > 2:
                if word.lower() not in stops():
                    if word.lower() not in count:
                        count[word.lower()] = 1
                    else:
                        count[word.lower()] += 1
        for word, ct in count.items():
            freq_dict[word] = float(ct) * 100.0 / float(length)
            if freq_dict[word] > 0.25:
                freq_list.append(word)
        return freq_list, flesch_from_list(freq_list)
    else:
        return None


def stops():
    '''
    Opens 'stops.csv' and creates a list of stop words to remove from texts.
    This function can be generalized for any stopword list if we make that
    a file input.

    Returns:
      stops_list: a list of stop words
    '''

    with open(r'stops.csv') as f:
        stops_list = []
        for row in csv.reader(f, delimiter=',', quoting=csv.QUOTE_NONE):
            stops_list += row
        return stops_list


def flesch_from_list(frequent_word_list):
    '''
    If we want advanced text, this function allows us to run Flesch Kincaid
    on a list of frequent words that we turn back into a string to process.

    Input:
      frequent_word_list: a list of frequent words

    Returns:
      This functions returns the Flesch Kincaid grade of the most
      frequent words.
    '''

    freq_words_string = " ".join(frequent_word_list)
    return textstat.flesch_kincaid_grade(freq_words_string)


def list_write(list_name, desired_filepath):
    '''
    A tool to save lists to a text file.

    Inputs:
      list_name (string): name of list to be saved
      desired_filepath (string): the filepath to save it in
    '''

    f = open(desired_filepath + '.txt', 'w')
    f.write(str(list_name))
    f.close()


def difficulty():
    '''
    Saves the whitepaper complexity scores of all coins into the non-time
    dependent CSV file.

    Input:
      csv_file (string): name of a csv with a column of coins called "Name"
    '''

    df_coins = pd.read_csv("Static_Params.csv")
    flesch_list = []
    for each in df_coins["Name"]:
        path = "whitepapers/" + each.split()[0] + ".txt"
        flesch = text(path)
        flesch_list.append(flesch)
    df_coins["White_Complexity"] = flesch_list
    df_coins.to_csv("Static_Params.csv")


def add_twitter_data():
    '''
    Combines the data for twitter mentions, total reddit subscribers, and
    whitepaper complexity scores into one CSV file.
    '''

    twitter_df = pd.read_csv("twitter.csv")
    static_df = pd.read_csv("Static_Params.csv")
    twitter_mentions = list(twitter_df["Twitter_Mentions"])
    static_df["Twitter_Mentions"] = twitter_mentions
    static_df.to_csv("Static_Params.csv")


def create_coin_dfs():
    '''
    Creates a dataframe for each of the 250 coins. Inputs all the dates from
    2016-01-01 to 2018-02-28 into one column and inserts 0 or 1 placeholder
    values for the other columns.
    '''

    start_date = datetime.date(2016, 1, 1)
    end_date = datetime.date(2018, 3, 1)
    date_generated = [start_date + datetime.timedelta(days=x) for x in \
    range(0, (end_date - start_date).days)]
    new_dict = pickle.load(open("all_data.p", "rb"))
    zero_values = [0 for i in range(len(date_generated))]
    one_values = [1 for i in range(len(date_generated))]
    for coin in new_dict.keys():
        dictionary = {"Dates": date_generated, "Price": zero_values, \
        "Volume": zero_values, "Reddit_Subscribers_Growth": one_values, \
        "Google_Trends": one_values, "Twitter_Mentions": one_values}
        df = pd.DataFrame(dictionary)
        df.to_csv("reddit_dfs/" + coin + "_reddit.csv", index=False)


def get_reddit_metrics_url(coin_name):
    '''
    Generates the url for the reddit metrics of each coin.

    Input:
      coin_name (string): the name of the coin

    Returns:
      url (string): the url of the Reddit metrics web page for each coin
    '''

    base_link = "http://redditmetrics.com/r/"
    url = base_link + coin_name
    return url


def get_subscriber_data(url):
    '''
    Requests the Reddit metrics url and scrapes the html for the total Reddit
    subscribers for a coin on each day from 2016-01-01 to 2018-02-28

    Input:
      url (string): the url of the Reddit metrics webpage

    Returns:
      all_dates (list): a list of all the dates on Reddit metrics
      subscriber_num (list): the number of Reddit subscribers for a coin on
      each date in the all_dates list
    '''

    r = requests.get(url)
    soup = BeautifulSoup(r.content, "lxml")
    scripts = soup.find_all("script")
    s = str(scripts[8])
    s = s.replace("\n", "")
    s = s.replace(" ", "")
    total_match = (re.findall(r"element:\'total-subscribers\',(.*?)\]", s))[0]
    dates = (re.findall(r"y:'(.*?)',a", total_match))
    num_subscribers = re.findall(r",a:(.*?)}", total_match)
    all_dates = []
    subscriber_num = []
    for d, num in zip(dates[:-1], num_subscribers[:-1]):
        year = int(d[:4])
        if d in all_dates:
            continue
        if year < 2016:
            continue
        if year == 2018:
            month = int(d.split('-')[1])
            if month > 2:
                continue
        all_dates.append(d)
        subscriber_num.append(num)
    return all_dates, subscriber_num


def get_reddit_metrics():
    '''
    Updates the dataframe for each coin by inputting the total Reddit
    subscribers for each date in the dataframes.
    '''

    coin_dict = pickle.load(open("all_data.p", "rb"))
    for key, val in coin_dict.items():
        if val != 0:
            url = get_reddit_metrics_url(key)
            all_dates, subscriber_num = get_subscriber_data(url)
            df = pd.read_csv("reddit_dfs/" + key + "_reddit.csv")
            difference_in_days = df.shape[0] - len(subscriber_num)
            subscriber_num += [val] * difference_in_days
            df["Reddit_Subscribers_Growth"] = subscriber_num
            df.to_csv("reddit_dfs/" + key + "_reddit.csv", index=False)


cwd = os.getcwd()


def generate_google_trends_url(coin_name):
    '''
    Generates the url for each coin's google trends webpage.

    Input:
      coin_name (string): the name of the coin

    Returns:
      url (string): the url of the coin's google trends page
    '''

    base_link = "https://trends.google.com/trends/explore?" + \
    "date=2015-12-27%202018-02-28&q="
    url = base_link + coin_name
    return url


def convert_date(date_string):
    '''
    Converts a date in a string to a datetime object.

    Input:
      date_string (string): the date

    Returns:
      d (object): a datetime object of the date_string
    '''

    year, month, day = date_string.split("-")
    d = datetime.date(int(year), int(month), int(day))
    return d


def google_trend_vals():
    '''
    Opens the CSV file downloaded from the google trends webpage and
    gets the google trends values for each day from 2016-01-01 to
    2018-02-28.

    Returns:
      new_vals (list): a list of the google trends values for a coin
      in the order of the dates we are focusing on
    '''

    file_name = "/Users/MichelleLiang/Downloads/multiTimeline.csv"
    with open(file_name, "rb") as f:
        data = f.read().decode()
    data = "\n".join(data.split("\n")[2:])
    with open(file_name, "wb") as f:
        f.write(data.encode())
    google_df = pd.read_csv\
    ("/Users/MichelleLiang/Downloads/multiTimeline.csv")
    week = google_df.columns[0]
    value = google_df.columns[1]
    all_dates = google_df[week]
    all_datetime = [convert_date(x) for x in all_dates]
    all_value = google_df[value]
    new_dates = []
    new_vals = []
    end_date = datetime.date(2018, 3, 1)
    start_date = datetime.date(2016, 1, 1)
    for date, val in zip(all_datetime, all_value):
        cur_week = [str(date + datetime.timedelta(days=x)) for x in range(7) \
        if date + datetime.timedelta(days=x) < end_date and date + \
        datetime.timedelta(days=x) >= start_date]
        cur_vals = [val] * len(cur_week)
        new_dates += cur_week
        for value in cur_vals:
            if value == "<1":
                value = 1
            new_vals.append(value)
    assert len(new_vals) == 790
    return new_vals


def download_and_save_data(coin_name):
    '''
    Updates the google trend column in the dataframe for the coin and
    reads it to another CSV file.

    Input:
      coin_name (string): the name of the coin
    '''

    url = generate_google_trends_url(coin_name)
    driver = webdriver.Chrome(cwd + "/chromedriver")
    driver.get(url)
    time.sleep(1)
    driver.find_element_by_xpath\
    ("//*[contains(text(), 'file_download')]").click()
    time.sleep(3 + random.uniform(0, 3))
    cur_coin_df = pd.read_csv("reddit_dfs/" + coin_name + "_reddit.csv")
    driver.quit()
    trend_values = google_trend_vals()
    cur_coin_df["Google_Trends"] = trend_values
    cur_coin_df.to_csv("All_Coin_dfs/" + coin_name + ".csv", index=False)
    os.remove("/Users/MichelleLiang/Downloads/multiTimeline.csv")


def get_google_trends_data():
    '''
    Calls the helper function to scrape the google trends website and update
    the dataframe of each coin.
    '''

    coin_df = pd.read_csv("Updated_Cryptocoins.csv")
    all_coins = [x.lower() for x in coin_df["Name"]][0:250]
    for i, coin in enumerate(all_coins):
        download_and_save_data(coin)


def convert_unix(unix_time):
    '''
    Converts unix time to datetime.date format

    Inputs:
      unix_time (int): time in unix format

    Returns:
      datetime.date(y,m,d) object
    '''

    return datetime.date.fromtimestamp(unix_time)


def gen_price_csv():
    '''
    Generates CSV files for all tickers mentioned. Pulls data from
    CryptoCompare API using requests, and then stores request as json to load
    as pandas df. CSV file output has 3 columns: Price, Volume, and Dates.
    '''

    coin_df = pd.read_csv("Cryptocoins.csv")
    coin_df = coin_df.head(NUM_COINS)
    coin_tickers = [ticker for ticker in coin_df["Symbol"]][0:NUM_COINS]
    coin_df = coin_df.set_index("Symbol")
    for ticker in coin_tickers:
        if ticker == "ACT" or ticker == "DEW" or ticker == "BIX" or \
        ticker == "UCASH" or ticker == "ECC":
            # these coins are not supported by CryptoCompare
            continue
        url = "https://min-api.cryptocompare.com/data/histoday?fsym=" + \
        ticker + "&tsym=USD&allData=True"
        r = requests.get(url)
        json_data = json.loads(r.text)
        df = pd.DataFrame(json_data["Data"])
        df["Dates"] = df["time"].apply(convert_unix)
        df["Price"] = df[["open", "close", "high", "low"]].mean(axis=1)
        df = df.set_index("Dates")
        if datetime.date(2016, 1, 1) in df.index:
            df = df.loc[datetime.date(2016, 1, 1):]
        df.rename(columns={"volumeto": "Volume"}, inplace=True)
        df = df.drop(["time", "open", "close", "high", "low", "volumefrom"], \
        axis=1)
        coin_name = coin_df.loc[ticker, "Name"]
        coin_tag = coin_name.lower()
        file_name = "historical_dfs" + coin_tag + "_hist.csv"
        df.to_csv(file_name, index=True, header=df.columns, mode = "a")


def add_historical_data():
    '''
    Takes the historical dataframes and adds the day to day price and volume
    to each coin's dataframe.
    '''

    coin_df = pd.read_csv("Cryptocoins.csv")
    updated_coin_df = pd.read_csv("Updated_Cryptocoins.csv")
    all_coins = [x.lower() for x in coin_df["Name"]][0:250]
    updated_coins = [x.lower() for x in updated_coin_df["Name"]][0:250]
    for i in range(0, 250):
        coin = all_coins[i]
        updated_coin = updated_coins[i]
        try:
            coin_df = pd.read_csv("All_Coin_dfs/" + updated_coin + ".csv")
            hist_df = pd.read_csv("historical_dfs/" + coin + "_hist.csv")
            hist_df = hist_df.drop(hist_df.index[-4:])
            price = list(hist_df["Price"])
            volume = list(hist_df["Volume"])
            dif = coin_df.shape[0] - len(price)
            price = [0] * dif + price
            volume = [0] * dif + volume
            coin_df["Price"] = price
            coin_df["Volume"] = volume
            coin_df.to_csv("All_Coin_dfs/" + updated_coin + ".csv", \
            index=False)
        except:
            continue


def create_df_for_leftover_coins():
    '''
    We were unable to to get the price and volume data for a few outlier
    coins with messed up json files. Thus, this function creates a final
    dataframe for the coins that have the historical dataframes and also
    makes the reddit dataframe into the final dataframe if the coin does
    not have a historical dataframe.
    '''

    updated_coin_df = pd.read_csv("Updated_Cryptocoins.csv")
    coin_df = pd.read_csv("Cryptocoins.csv")
    all_coins = [x.lower() for x in coin_df["Name"]][0:250]
    all_updated_coins = [x.lower() for x in updated_coin_df["Name"]][0:250]
    for i in range(0, 250):
        coin = all_coins[i]
        updated_coin = all_updated_coins[i]
        try:
            df = pd.read_csv("historical_dfs/" + coin + "_hist.csv")
            df = pd.read_csv("All_Coin_dfs/" + updated_coin + ".csv")
        except:
            df = pd.read_csv("reddit_dfs/" + updated_coin + "_reddit.csv")

        df.to_csv("final_coins/" + updated_coin + ".csv", index=False)
